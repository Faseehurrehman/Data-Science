{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441c5697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\A\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d98e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv',encoding='ISO-8859-1')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01840d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Clenaing\n",
    "# 2. EDA\n",
    "# 3. Text Preprocessing (stemming, vectorization,removal of stop words)\n",
    "# 4. Model Building\n",
    "# 5. Model Evaluation\n",
    "# 6. Improvements\n",
    "# 7. Converting into Website\n",
    "# 8. Deployment of Website on Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7ad5d",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee535a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop last 3 columns\n",
    "\n",
    "df.drop(columns = ['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eecb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column names\n",
    "df.rename(columns = {'v1':'target','v2':'message'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254689cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying LabelEncoder on target column\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bff818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd295503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a020d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "df = df.drop_duplicates(keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57151ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbc15a",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f953fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.target.value_counts())\n",
    "df.target.value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['target'].value_counts()/df['target'].count()*100)\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.pie(df['target'].value_counts(),labels = ['ham','spam'],autopct = '%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621126b6",
   "metadata": {},
   "source": [
    "<b> It shows data is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed7d1f",
   "metadata": {},
   "source": [
    "### Creation of Basic Features from 'message' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('I am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bb52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of message - Number of Characters in message\n",
    "df['num_characters'] =  df['message'].apply(len) # OR df['message'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54447424",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['message'].apply(lambda x: len(nltk.word_tokenize(x)))  # OR df['message'].apply(lambda row: len(row.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0473e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Sentences\n",
    "\n",
    "df['num_sentences'] = df['message'].apply(lambda x:len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95284a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00519149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham messages\n",
    "\n",
    "df[df['target']==0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43258e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spam messages\n",
    "\n",
    "df[df['target']==1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[df['target']==0]['num_characters'])\n",
    "sns.histplot(df[df['target']==1]['num_characters'],color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7d7a0",
   "metadata": {},
   "source": [
    "<b> It shows number of characters in most of ham messages are lesser than that of spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af23035",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[df['target']==0]['num_words'])\n",
    "sns.histplot(df[df['target']==1]['num_words'],color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4851a",
   "metadata": {},
   "source": [
    "<b> It shows number of words in most of ham messages are lesser than that of spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518880ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[df['target']==0]['num_sentences'])\n",
    "sns.histplot(df[df['target']==1]['num_sentences'],color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b8e7a4",
   "metadata": {},
   "source": [
    "<b> It shows number of sentences in most of ham messages are lesser than that of spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.pairplot(df,hue='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a64ff",
   "metadata": {},
   "source": [
    "<b> It shows data has outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c605906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show correlation in data\n",
    "plt.figure(figsize = (15,12))\n",
    "sns.heatmap(df.corr(),annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79412707",
   "metadata": {},
   "source": [
    "<b> It shows input variables(such as num_characters, num_words and num_sentences) have strong correlation with each other so multicollinearity problem exists. We will have to keep one variable (which will be num_characters as it has more correlation with target variable than other columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2898176",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1464c",
   "metadata": {},
   "source": [
    "i. Lower case </br>\n",
    "ii. Tokenization</br>\n",
    "iii. Removing Special Characters</br>\n",
    "iv. Removing Stop Words and Punctuation</br>\n",
    "v. Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b976ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stop words in English\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation marks\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "# lemma = nltk.WordNetLemmatizer() # define lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = text.lower() # Lowercasing\n",
    "    text = nltk.word_tokenize(text) # Tokenization\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", str(text)) # Remove non-letters   If this error 'expected string or bytes-like object' occurs then replace re.sub(\"[^a-zA-Z]\", \" \", text) with re.sub(\"[^a-zA-Z]\", \" \", str(text))\n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # Remove punctuation\n",
    "    text = re.sub(' +', ' ',text) # Remove extra space\n",
    "    text = text.strip() # remove whitespaces\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words(\"english\")]) # Remove stop words\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()]) # OR ' '.join([lemma.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_text(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove url links\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca94a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_url('I am 12, www.google.com  23n ? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming WordCloud which will highligh important words in both ham and spam\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77786daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spam messages\n",
    "spam_wc = wc.generate(df[df['target'] == 1]['message'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7754ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85177",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ham messages\n",
    "ham_wc = wc.generate(df[df['target'] == 0]['message'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d78eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(ham_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae432daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top words in each category\n",
    "### Spam category\n",
    "spam_corpus = []\n",
    "for msg in df[df['target']==1]['message'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(spam_corpus) # Dictionary will be craeted showing how many times each word has appeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(spam_corpus).most_common(30) # Most common words in spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e78d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(spam_corpus).most_common(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd52940",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(spam_corpus).most_common(30))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ham category\n",
    "\n",
    "ham_corpus = []\n",
    "for msg in df[df['target']==0]['message'].tolist():\n",
    "    for word in msg.split():\n",
    "        ham_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f27a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(ham_corpus).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7ee28",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c211b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform(df['message']).toarray() # Use toarray() to convert sparse array into dense array\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit_transform(df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0982520",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # total 5169 sms, and 6216 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0490e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5985f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On textual based data,naive bayes algorithm performs better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a73873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebfb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred1))\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print(precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a98c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print(confusion_matrix(y_test,y_pred3))\n",
    "print(precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7f41c",
   "metadata": {},
   "source": [
    "<b> In case of imbalanced dataset, precision score matters alot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Tfidf Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features = 3000)\n",
    "\n",
    "X = tfidf.fit_transform(df['message']).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6104f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying MinMaxScaling because standardscaling also gives negative values which naive bayes does not accept\n",
    "\n",
    "\"\"\"from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the num_character col to X\n",
    "# X = np.hstack((X,df['num_characters'].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca210b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred1))\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print(precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd2d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print(confusion_matrix(y_test,y_pred3))\n",
    "print(precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657cde2f",
   "metadata": {},
   "source": [
    "<b> This is performing best, maximum precision score (no false positive) and good accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c02f3",
   "metadata": {},
   "source": [
    "<b> So, we have choosen tfidf and mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing results with other algorithms\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9102a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    \n",
    "    return accuracy,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e336ca3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('SVC: ',train_classifier(svc,X_train,y_train,X_test,y_test))\n",
    "print('KNeighborsClassifier: ',train_classifier(knc,X_train,y_train,X_test,y_test))\n",
    "print('MultinomialNB: ',train_classifier(mnb,X_train,y_train,X_test,y_test))\n",
    "print('DecisionTreeClassifier: ',train_classifier(dtc,X_train,y_train,X_test,y_test))\n",
    "print('LogisticRegression: ',train_classifier(lrc,X_train,y_train,X_test,y_test))\n",
    "print('RandomForestClassifier: ',train_classifier(rfc,X_train,y_train,X_test,y_test))\n",
    "print('AdaBoostClassifier: ',train_classifier(abc,X_train,y_train,X_test,y_test))\n",
    "print('BaggingClassifier: ',train_classifier(bc,X_train,y_train,X_test,y_test))\n",
    "print('ExtraTreesClassifier: ',train_classifier(etc,X_train,y_train,X_test,y_test))\n",
    "print('GradientBoostingClassifier: ',train_classifier(gbdt,X_train,y_train,X_test,y_test))\n",
    "print('XGBClassifier: ',train_classifier(xgb,X_train,y_train,X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70015a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "\n",
    "for name,clf in clfs.items():\n",
    "    \n",
    "    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    print(\"For \",name)\n",
    "    print(\"Accuracy - \",current_accuracy)\n",
    "    print(\"Precision - \",current_precision)\n",
    "    \n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df1 = pd.melt(performance_df, id_vars = \"Algorithm\")\n",
    "performance_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe30353",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = 'Algorithm', y='value', \n",
    "               hue = 'variable',data=performance_df1, kind='bar',height=5)\n",
    "plt.ylim(0.5,1.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f889ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model improve\n",
    "# 1. Change the max_features parameter of TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)\n",
    "#temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4050b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = performance_df.merge(temp_df,on='Algorithm')\n",
    "#new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6569607",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#new_df_scaled = new_df.merge(performance_df,on='Algorithm')\n",
    "#new_df_scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_d = new_df_scaled.merge(performance_df,on='Algorithm')\n",
    "#new_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af6200",
   "metadata": {},
   "source": [
    "<b> Results show scaling is not favorable.Only tfidf with max_features =3000 gives better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)\n",
    "mnb = MultinomialNB()\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc93121",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a32c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting.predict(X_test)\n",
    "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision\",precision_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stacking\n",
    "estimators=[('svm', svc), ('nb', mnb), ('et', etc)]\n",
    "final_estimator=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed45457",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision\",precision_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b600d",
   "metadata": {},
   "source": [
    "<b> We will go for tfidf and mnb, keeping in mind precision score first sicnce this is imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ef57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb,open('model.pkl','wb'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input statment\n",
    "test_review = ['This is a bad movie'] \n",
    "\n",
    "# convert to number\n",
    "test_vector = tfidf.transform(test_review)\n",
    "test_vector = test_vector.toarray()\n",
    "\n",
    "## encodeing predict class\n",
    "text_predict_class = le.inverse_transform(mnb.predict(test_vector))\n",
    "print(test_review[0], 'is: ',text_predict_class[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
