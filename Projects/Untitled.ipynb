{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc2b8c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Take the action and observe the new state and reward\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Render the environment\u001b[39;00m\n\u001b[0;32m     16\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Run the simulation for 200 steps\n",
    "for _ in range(200):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    # Take the action and observe the new state and reward\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    # Render the environment\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1626183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Run the simulation for 200 steps\n",
    "for _ in range(200):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    # Take the action and observe the new state and reward\n",
    "    next_state, reward, done, *extra_info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    # Render the environment\n",
    "    env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b47cd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create a figure to display the animation\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Define the function to update the animation\n",
    "def update(num):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    # Take the action and observe the new state and reward\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    # Clear the previous frame\n",
    "    ax.clear()\n",
    "    # Plot the new frame\n",
    "    ax.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "# Create the animation\n",
    "ani = animation.FuncAnimation(fig, update, interval=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23cbd903",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Monitor' from 'gym.wrappers' (C:\\Users\\Admin\\anaconda3\\envs\\A\\lib\\site-packages\\gym\\wrappers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Monitor\n\u001b[0;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m Monitor(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gym-results\u001b[39m\u001b[38;5;124m\"\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Monitor' from 'gym.wrappers' (C:\\Users\\Admin\\anaconda3\\envs\\A\\lib\\site-packages\\gym\\wrappers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1cccea9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.wrappers' has no attribute 'Monitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a monitor wrapper to record the video\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mwrappers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMonitor\u001b[49m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gym-results\u001b[39m\u001b[38;5;124m\"\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Define the number of states and actions\u001b[39;00m\n\u001b[0;32m     12\u001b[0m num_states \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym.wrappers' has no attribute 'Monitor'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create a monitor wrapper to record the video\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "\n",
    "# Define the number of states and actions\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Define the Q-table and initialize it to random values\n",
    "q_table = np.random.rand(num_states, num_actions)\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "\n",
    "# Define the number of episodes\n",
    "num_episodes = 1000\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "\n",
    "    # Run the simulation for 200 steps\n",
    "    for step in range(200):\n",
    "        # Choose an action based on the current state\n",
    "        action = np.argmax(q_table[state])\n",
    "\n",
    "        # Take the action and observe the new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update the Q-value for the current state and action\n",
    "        q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Print the episode number every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3542d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'video_recorder' from 'gym.wrappers' (C:\\Users\\Admin\\anaconda3\\envs\\A\\lib\\site-packages\\gym\\wrappers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m video_recorder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create the CartPole environment\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'video_recorder' from 'gym.wrappers' (C:\\Users\\Admin\\anaconda3\\envs\\A\\lib\\site-packages\\gym\\wrappers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import video_recorder\n",
    "import numpy as np\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create a monitor wrapper to record the video\n",
    "env = video_recorder.VideoRecorder(env, \"./gym-results\", enabled=True)\n",
    "\n",
    "# Define the number of states and actions\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Define the Q-table and initialize it to random values\n",
    "q_table = np.random.rand(num_states, num_actions)\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "\n",
    "# Define the number of episodes\n",
    "num_episodes = 1000\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "\n",
    "    # Run the simulation for 200 steps\n",
    "    for step in range(200):\n",
    "        # Choose an action based on the current state\n",
    "        action = np.argmax(q_table[state])\n",
    "\n",
    "        # Take the action and observe the new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update the Q-value for the current state and action\n",
    "        q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Print the episode number every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ede0039",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.wrappers' has no attribute 'Monitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a monitor wrapper to record the video\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mwrappers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMonitor\u001b[49m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gym-results\u001b[39m\u001b[38;5;124m\"\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Define the number of states and actions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m num_states \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym.wrappers' has no attribute 'Monitor'"
     ]
    }
   ],
   "source": [
    "!pip install gym_recording\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create a monitor wrapper to record the video\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "\n",
    "# Define the number of states and actions\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Define the Q-table and initialize it to random values\n",
    "q_table = np.random.rand(num_states, num_actions)\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "\n",
    "# Define the number of episodes\n",
    "num_episodes = 1000\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "\n",
    "    # Run the simulation for 200 steps\n",
    "    for step in range(200):\n",
    "        # Choose an action based on the current state\n",
    "        action = np.argmax(q_table[state])\n",
    "\n",
    "        # Take the action and observe the new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update the Q-value for the current state and action\n",
    "        q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Print the episode number every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4668824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
